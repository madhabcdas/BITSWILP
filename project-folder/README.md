Breast Cancer Classification – Machine Learning Model Evaluation

a. **Problem Statement**
Breast cancer is one of the most common cancers affecting women worldwide. Early detection plays a crucial role in improving survival rates.
This project focuses on building and evaluating multiple machine learning models to classify tumors as Benign (B) or Malignant (M) using the Breast Cancer Wisconsin Diagnostic Dataset.
The goal is to compare six ML models based on standard evaluation metrics and identify which model performs best for this classification task.

b. **Dataset Description**
The dataset contains 30 numerical features computed from digitized images of breast mass cell nuclei.
These features describe characteristics such as:
- Radius
- Texture
- Perimeter
- Area
- Smoothness
- Compactness
- Concavity
- Symmetry
- Fractal dimension
Each feature is provided in three forms:
- Mean
- Standard error (SE)
- Worst (largest value)
The target variable is:
- Diagnosis
- B = Benign (0)
- M = Malignant (1)
The dataset used for evaluation is uploaded by the user through the Streamlit interface.

c. **Models Used**
The following six machine learning models were trained and evaluated:
- Logistic Regression
- Decision Tree Classifier
- k‑Nearest Neighbour (kNN)
- Naive Bayes (GaussianNB)
- Random Forest (Ensemble)
- XGBoost (Ensemble)
Evaluation metrics computed for each model:
- Accuracy
- AUC
- Precision
- Recall
- F1 Score
- MCC (Matthews Correlation Coefficient)

**Comparison Table of Evaluation Metrics**(Filled the table below using the metrics generated by Streamlit app)

| ML Model Name            | Accuracy | AUC | Precision | Recall | F1  | MCC |
|--------------------------|----------|-----|-----------|--------|-----|-----|
| Logistic Regression      | 0.9824253075571178|0.9973574335394535|0.9950980392156863|0.9575471698113207|0.9759615384615384|0.9625612906517853|
| Decision Tree            |0.984182776801406|0.9816473759315048|0.9856459330143541|0.9716981132075472|0.9786223277909739|0.9661328283092322|
| kNN                      |0.984182776801406|0.9816473759315048|0.9856459330143541|0.9716981132075472|0.9786223277909739|0.9661328283092322|
| Naive Bayes              |0.9420035149384886|0.9893768828286031|0.9543147208121827|0.8867924528301887|0.9193154034229829|0.8755768261309823|
| Random Forest (Ensemble) |0.9947275922671354|0.9995970086147666|1|0.9858490566037735|0.9928741092636579|0.9887535829511528|
| XGBoost (Ensemble)       |0.9947275922671354|0.9995970086147666|1|0.9858490566037735|0.9928741092636579|0.9887535829511528|
|--------------------------|----------|-----|-----------|--------|-----|-----|






d. **Observations on Model Performance**(Used the insights from evaluation results to fill the table below)
|ML Model Name|Observation about model performance|
|--------------------------|----------|
| Logistic Regression      | The Logistic Regression model performed extremely well, achieving high accuracy, AUC, and precision. Its recall is slightly lower compared to ensemble models, indicating that it missed a few malignant cases. Overall, it provides a strong and reliable baseline model with balanced performance across all metrics.|
| Decision Tree            |The Decision Tree model showed strong performance with high accuracy and recall. However, its AUC is slightly lower than Logistic Regression and ensemble models, suggesting that it may not generalize as smoothly. Still, it performs consistently well and captures non‑linear patterns effectively|
| kNN                      |The kNN model produced identical results to the Decision Tree in this evaluation, indicating strong predictive capability. It maintains high precision and recall, but like the Decision Tree, its AUC is slightly lower than the top-performing ensemble models. It performs well but is more sensitive to scaling and neighborhood structure.|
| Naive Bayes              |Naive Bayes showed comparatively lower performance across all metrics. Its recall is notably lower, meaning it misclassified more malignant cases than other models. This is expected because Naive Bayes assumes feature independence, which does not hold well for this dataset. It remains fast and simple but is not the best choice for this problem.|
| Random Forest (Ensemble) |Random Forest delivered outstanding performance, achieving near-perfect accuracy, AUC, and precision. Its recall is extremely high, meaning it correctly identifies almost all malignant cases. The MCC score is also the highest, indicating excellent overall predictive power. This model is one of the best performers in the evaluation.|
| XGBoost (Ensemble)       |XGBoost matched the performance of Random Forest in all metrics, achieving near-perfect results. Its ability to capture complex patterns and reduce bias makes it one of the strongest models in this comparison. Like Random Forest, it demonstrates exceptional reliability and robustness.|


e. **How to Use the Streamlit App**
- Upload the test dataset (CSV format).
- The app validates the schema and displays a preview.
- Select a machine learning model from the dropdown.
- The app displays:
- Evaluation metrics
- Confusion matrix
- Classification report
- Use the results to fill the comparison tables above.

f. **Technologies Used**
- Python
- Streamlit
- Scikit‑learn
- XGBoost
- Seaborn & Matplotlib
- Pandas & NumPy

g. **Author**
_Madhab Chandra Das_
BITS Pilani WILP – Machine Learning Lab Assignment
2025ab05151@wilp.bits-pilani.ac.in
